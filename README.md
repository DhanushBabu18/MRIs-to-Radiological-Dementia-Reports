# MRIs to Radiological Dementia Reports

Convolutional neural network (CNN) have become the standard for neuroimaging analysis, and Large language models (LLMs) are increasingly used to supplement them. But LLMs' intractable reasoning and hallucinations limit clinical usage. We present a framework that validates CNN predictions and steers LLM explanations by combining deterministic rules with retrieval-augmented generation (RAG). From MRIs we compute covariate-adjusted scores for morphology and CNN's relevance. A rule module encodes anatomical constraints and threshold scores to validate CNN plausibility and summarize pathology. For steerable justifications, LLM is conditioned on diagnostic guidelines via RAG. We introduce a falsifiable taxonomy, turning a vague notion of hallucination into actionable scenarios. On N=3,408 scans, a validator-guided self-reflection mechanism improved alignment with the rule baseline. However, hallucinations persist despite having diagnostic-guideline conditioned RAG module, where between ~34\% to ~50\% of claims were hallucinated in justifications generated for pathological samples. Overall, our proof-of-concept study shows that integrating deterministic guardrails with RAG improves justification consistency and auditability but does not yet eliminate hallucinations.

